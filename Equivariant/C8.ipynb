{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import model as eq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "    \n",
    "def metric(y_true, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return auc\n",
    "\n",
    "def straightner(a):\n",
    "    A = np.zeros((a[0].shape[0]*len(a)))\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    for i in range(len(a)):\n",
    "        start_index = i*a[0].shape[0]\n",
    "        end_index = start_index+a[0].shape[0]\n",
    "        A[start_index:end_index] = a[i]\n",
    "    return A\n",
    "\n",
    "def predictor(outputs):\n",
    "    return np.argmax(outputs, axis = 1)\n",
    "\n",
    "def trainer():\n",
    "    model = eq_model.model(channels=3,N=8, group = \"cyclic\")\n",
    "    \n",
    "    train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    \n",
    "    dataset_Train = datasets.ImageFolder('./Data/Train/', transform=train_transform)\n",
    "    dataset_Test = datasets.ImageFolder('./Data/Test/', transform =test_transform)\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_Train, batch_size=64, shuffle=True, drop_last = True, num_workers=4, pin_memory = True)\n",
    "    dataloader_test = torch.utils.data.DataLoader(dataset_Test, batch_size=64, shuffle=True, drop_last = True, num_workers=4, pin_memory = True)    \n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', verbose = True,threshold = 0.0001,patience = 3, factor = 0.5)\n",
    "    \n",
    "    model = model.to(\"cuda:1\")\n",
    "    \n",
    "\n",
    "\n",
    "    import wandb\n",
    "    wandb.login(key=\"cb53927c12bd57a0d943d2dedf7881cfcdcc8f09\")\n",
    "    wandb.init(\n",
    "        project = \"Equivariant\",\n",
    "        name = \"C8\"\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    #--------------------------\n",
    "    wandb.watch(model, log_freq=50)\n",
    "    #---------------------------\n",
    "    w_intr = 50\n",
    "\n",
    "    for epoch in range(20):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        train_steps = 0\n",
    "        test_steps = 0\n",
    "        label_list = []\n",
    "        outputs_list = []\n",
    "        train_auc = 0\n",
    "        test_auc = 0\n",
    "        model.train()\n",
    "        for image, label in tqdm(dataloader_train):\n",
    "            image = image.to(\"cuda:1\")\n",
    "            label = label.to(\"cuda:1\")\n",
    "            with torch.no_grad():\n",
    "                image = nn.functional.pad(image, (2,1,2,1))\n",
    "            #optimizer.zero_grad()\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "              outputs = model(image)\n",
    "              loss = criterion(outputs, label.float())\n",
    "            label_list.append(label.detach().cpu().numpy())\n",
    "            outputs_list.append(outputs.detach().cpu().numpy())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            if train_steps%w_intr == 0:\n",
    "                 wandb.log({\"loss\": loss.item()})\n",
    "        with torch.no_grad():\n",
    "            label_list = straightner(label_list)\n",
    "            outputs_list = straightner(outputs_list)\n",
    "            train_auc = metric(label_list, outputs_list) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #-------------------------------------------------------------------\n",
    "        model.eval()\n",
    "        label_list = []\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for image, label in tqdm(dataloader_test):\n",
    "                image = image.to(\"cuda:1\")\n",
    "                label = label.to(\"cuda:1\")\n",
    "                image = nn.functional.pad(image, (2,1,2,1))\n",
    "                outputs = model(image)\n",
    "                loss = criterion(outputs, label.float())\n",
    "                label_list.append(label.detach().cpu().numpy())\n",
    "                outputs_list.append(outputs.detach().cpu().numpy())\n",
    "                val_loss += loss.item()\n",
    "                test_steps +=1\n",
    "                if test_steps%w_intr == 0:\n",
    "                 wandb.log({\"val_loss\": loss.item()})\n",
    "            label_list = straightner(label_list)\n",
    "            outputs_list = straightner(outputs_list)\n",
    "            test_auc = metric(label_list, outputs_list)\n",
    "\n",
    "        train_loss = train_loss/train_steps\n",
    "        val_loss = val_loss/ test_steps\n",
    "        \n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"Epoch No\" , epoch)\n",
    "        print(\"The Training loss of the epoch, \",train_loss)\n",
    "        print(\"The Training AUC of the epoch,  %.5f\"%train_auc)\n",
    "        print(\"The validation loss of the epoch, \",val_loss)\n",
    "        print(\"The validation AUC of the epoch, %.5f\"%test_auc)\n",
    "        print(\"----------------------------------------------------\")\n",
    "        PATH = f\"model_Epoch_{epoch}.pt\"\n",
    "#         torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'scheduler': scheduler.state_dict()\n",
    "#                 }, PATH)\n",
    "        scheduler.step(test_auc)\n",
    "        curr_lr = scheduler._last_lr[0]\n",
    "        wandb.log({\"Train_auc_epoch\": train_auc,\n",
    "                  \"Epoch\": epoch,\n",
    "                  \"Val_auc_epoch\": test_auc,\n",
    "                  \"Train_loss_epoch\": train_loss,\n",
    "                  \"Val_loss_epoch\": val_loss,\n",
    "                  \"Lr\": curr_lr}\n",
    "                 )\n",
    "        gc.collect()\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdc250601\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diptarko/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/diptarko/G23/wandb/run-20230402_092839-h7k0lrqi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dc250601/Equivariant/runs/h7k0lrqi' target=\"_blank\">C8</a></strong> to <a href='https://wandb.ai/dc250601/Equivariant' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dc250601/Equivariant' target=\"_blank\">https://wandb.ai/dc250601/Equivariant</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dc250601/Equivariant/runs/h7k0lrqi' target=\"_blank\">https://wandb.ai/dc250601/Equivariant/runs/h7k0lrqi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:09<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 0\n",
      "The Training loss of the epoch,  0.5769874441041344\n",
      "The Training AUC of the epoch,  0.76896\n",
      "The validation loss of the epoch,  0.5777225972592146\n",
      "The validation AUC of the epoch, 0.77869\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:16<00:00,  2.57it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 1\n",
      "The Training loss of the epoch,  0.5637606028338958\n",
      "The Training AUC of the epoch,  0.78194\n",
      "The validation loss of the epoch,  0.5769779804794267\n",
      "The validation AUC of the epoch, 0.78502\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:13<00:00,  2.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:00<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 2\n",
      "The Training loss of the epoch,  0.5587452794457304\n",
      "The Training AUC of the epoch,  0.78676\n",
      "The validation loss of the epoch,  0.5682234488684555\n",
      "The validation AUC of the epoch, 0.78650\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:09<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 3\n",
      "The Training loss of the epoch,  0.5544563652968955\n",
      "The Training AUC of the epoch,  0.79097\n",
      "The validation loss of the epoch,  0.5547332948651807\n",
      "The validation AUC of the epoch, 0.79140\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:08<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:00<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 4\n",
      "The Training loss of the epoch,  0.5514984331425579\n",
      "The Training AUC of the epoch,  0.79369\n",
      "The validation loss of the epoch,  0.5608880225954385\n",
      "The validation AUC of the epoch, 0.79259\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:08<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 5\n",
      "The Training loss of the epoch,  0.5475334376096725\n",
      "The Training AUC of the epoch,  0.79737\n",
      "The validation loss of the epoch,  0.566116482430491\n",
      "The validation AUC of the epoch, 0.79217\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:08<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 6\n",
      "The Training loss of the epoch,  0.5439946938862745\n",
      "The Training AUC of the epoch,  0.80059\n",
      "The validation loss of the epoch,  0.5537958133494717\n",
      "The validation AUC of the epoch, 0.79524\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:09<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 7\n",
      "The Training loss of the epoch,  0.5391037986196321\n",
      "The Training AUC of the epoch,  0.80506\n",
      "The validation loss of the epoch,  0.5705277750546904\n",
      "The validation AUC of the epoch, 0.79359\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:08<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 8\n",
      "The Training loss of the epoch,  0.5340814914168983\n",
      "The Training AUC of the epoch,  0.80941\n",
      "The validation loss of the epoch,  0.5681234809173935\n",
      "The validation AUC of the epoch, 0.78888\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:08<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 9\n",
      "The Training loss of the epoch,  0.5256443499833688\n",
      "The Training AUC of the epoch,  0.81652\n",
      "The validation loss of the epoch,  0.5784637078471567\n",
      "The validation AUC of the epoch, 0.78996\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:08<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 10\n",
      "The Training loss of the epoch,  0.5141777230576537\n",
      "The Training AUC of the epoch,  0.82599\n",
      "The validation loss of the epoch,  0.58060413770292\n",
      "The validation AUC of the epoch, 0.77505\n",
      "----------------------------------------------------\n",
      "Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:09<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 11\n",
      "The Training loss of the epoch,  0.47775654088834235\n",
      "The Training AUC of the epoch,  0.85311\n",
      "The validation loss of the epoch,  0.6268453497311165\n",
      "The validation AUC of the epoch, 0.75961\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:09<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 12\n",
      "The Training loss of the epoch,  0.44244907789531795\n",
      "The Training AUC of the epoch,  0.87601\n",
      "The validation loss of the epoch,  0.6413778079652238\n",
      "The validation AUC of the epoch, 0.75823\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:10<00:00,  2.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 13\n",
      "The Training loss of the epoch,  0.403695464305494\n",
      "The Training AUC of the epoch,  0.89814\n",
      "The validation loss of the epoch,  0.6646416096851744\n",
      "The validation AUC of the epoch, 0.75113\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:11<00:00,  2.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 14\n",
      "The Training loss of the epoch,  0.3586579474909552\n",
      "The Training AUC of the epoch,  0.92034\n",
      "The validation loss of the epoch,  0.7751011482600508\n",
      "The validation AUC of the epoch, 0.73698\n",
      "----------------------------------------------------\n",
      "Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:16<00:00,  2.57it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 15\n",
      "The Training loss of the epoch,  0.26828875575942557\n",
      "The Training AUC of the epoch,  0.95521\n",
      "The validation loss of the epoch,  0.8766856230538467\n",
      "The validation AUC of the epoch, 0.71933\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:21<00:00,  2.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 16\n",
      "The Training loss of the epoch,  0.21755408948694152\n",
      "The Training AUC of the epoch,  0.96997\n",
      "The validation loss of the epoch,  0.9854201438098118\n",
      "The validation AUC of the epoch, 0.72323\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:14<00:00,  2.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 17\n",
      "The Training loss of the epoch,  0.1746228604866513\n",
      "The Training AUC of the epoch,  0.98017\n",
      "The validation loss of the epoch,  1.1756191618141087\n",
      "The validation AUC of the epoch, 0.70791\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:14<00:00,  2.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 18\n",
      "The Training loss of the epoch,  0.13988634393304245\n",
      "The Training AUC of the epoch,  0.98695\n",
      "The validation loss of the epoch,  1.2521948792468542\n",
      "The validation AUC of the epoch, 0.70322\n",
      "----------------------------------------------------\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [11:12<00:00,  2.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [01:01<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 19\n",
      "The Training loss of the epoch,  0.0891926135279067\n",
      "The Training AUC of the epoch,  0.99402\n",
      "The validation loss of the epoch,  1.4763251939039121\n",
      "The validation AUC of the epoch, 0.70019\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>Lr</td><td>██████████▄▄▄▄▂▂▂▂▁▁</td></tr><tr><td>Train_auc_epoch</td><td>▁▁▂▂▂▂▂▂▂▂▃▄▄▅▆▇▇███</td></tr><tr><td>Train_loss_epoch</td><td>███████▇▇▇▇▇▆▆▅▄▃▂▂▁</td></tr><tr><td>Val_auc_epoch</td><td>▇▇▇███████▇▅▅▅▄▂▃▂▁▁</td></tr><tr><td>Val_loss_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▄▆▆█</td></tr><tr><td>loss</td><td>▇▆▆▇█▇▇█▇▇▇▇█▇▆▆▆▆▆▇▆▅▅▆▄▆▄▄▃▄▄▃▂▃▂▃▃▃▁▁</td></tr><tr><td>val_loss</td><td>▂▂▃▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁▂▂▃▂▂▂▂▁▂▂▃▂▅▆▄▄▆▅▆█▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>19</td></tr><tr><td>Lr</td><td>0.00013</td></tr><tr><td>Train_auc_epoch</td><td>0.99402</td></tr><tr><td>Train_loss_epoch</td><td>0.08919</td></tr><tr><td>Val_auc_epoch</td><td>0.70019</td></tr><tr><td>Val_loss_epoch</td><td>1.47633</td></tr><tr><td>loss</td><td>0.04378</td></tr><tr><td>val_loss</td><td>1.36188</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">C8</strong> at: <a href='https://wandb.ai/dc250601/Equivariant/runs/h7k0lrqi' target=\"_blank\">https://wandb.ai/dc250601/Equivariant/runs/h7k0lrqi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230402_092839-h7k0lrqi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
