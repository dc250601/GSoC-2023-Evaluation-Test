{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import model as eq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "    \n",
    "def metric(y_true, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return auc\n",
    "\n",
    "def straightner(a):\n",
    "    A = np.zeros((a[0].shape[0]*len(a)))\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    for i in range(len(a)):\n",
    "        start_index = i*a[0].shape[0]\n",
    "        end_index = start_index+a[0].shape[0]\n",
    "        A[start_index:end_index] = a[i]\n",
    "    return A\n",
    "\n",
    "def predictor(outputs):\n",
    "    return np.argmax(outputs, axis = 1)\n",
    "\n",
    "def trainer():\n",
    "    model = eq_model.model(channels=3,N=8, group = \"dihyderal\")\n",
    "    \n",
    "    train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    \n",
    "    dataset_Train = datasets.ImageFolder('./Data/Train/', transform=train_transform)\n",
    "    dataset_Test = datasets.ImageFolder('./Data/Test/', transform =test_transform)\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_Train, batch_size=64, shuffle=True, drop_last = True, num_workers=4, pin_memory = True)\n",
    "    dataloader_test = torch.utils.data.DataLoader(dataset_Test, batch_size=64, shuffle=True, drop_last = True, num_workers=4, pin_memory = True)    \n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', verbose = True,threshold = 0.0001,patience = 3, factor = 0.5)\n",
    "    \n",
    "    model = model.to(\"cuda:3\")\n",
    "    \n",
    "\n",
    "\n",
    "    import wandb\n",
    "    wandb.login(key=\"cb53927c12bd57a0d943d2dedf7881cfcdcc8f09\")\n",
    "    wandb.init(\n",
    "        project = \"Equivariant\",\n",
    "        name = \"D8\"\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    #--------------------------\n",
    "    wandb.watch(model, log_freq=50)\n",
    "    #---------------------------\n",
    "    w_intr = 50\n",
    "\n",
    "    for epoch in range(20):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        train_steps = 0\n",
    "        test_steps = 0\n",
    "        label_list = []\n",
    "        outputs_list = []\n",
    "        train_auc = 0\n",
    "        test_auc = 0\n",
    "        model.train()\n",
    "        for image, label in tqdm(dataloader_train):\n",
    "            image = image.to(\"cuda:3\")\n",
    "            label = label.to(\"cuda:3\")\n",
    "            with torch.no_grad():\n",
    "                image = nn.functional.pad(image, (2,1,2,1))\n",
    "            #optimizer.zero_grad()\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "              outputs = model(image)\n",
    "              loss = criterion(outputs, label.float())\n",
    "            label_list.append(label.detach().cpu().numpy())\n",
    "            outputs_list.append(outputs.detach().cpu().numpy())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            if train_steps%w_intr == 0:\n",
    "                 wandb.log({\"loss\": loss.item()})\n",
    "        with torch.no_grad():\n",
    "            label_list = straightner(label_list)\n",
    "            outputs_list = straightner(outputs_list)\n",
    "            train_auc = metric(label_list, outputs_list) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #-------------------------------------------------------------------\n",
    "        model.eval()\n",
    "        label_list = []\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for image, label in tqdm(dataloader_test):\n",
    "                image = image.to(\"cuda:3\")\n",
    "                label = label.to(\"cuda:3\")\n",
    "                image = nn.functional.pad(image, (2,1,2,1))\n",
    "                outputs = model(image)\n",
    "                loss = criterion(outputs, label.float())\n",
    "                label_list.append(label.detach().cpu().numpy())\n",
    "                outputs_list.append(outputs.detach().cpu().numpy())\n",
    "                val_loss += loss.item()\n",
    "                test_steps +=1\n",
    "                if test_steps%w_intr == 0:\n",
    "                 wandb.log({\"val_loss\": loss.item()})\n",
    "            label_list = straightner(label_list)\n",
    "            outputs_list = straightner(outputs_list)\n",
    "            test_auc = metric(label_list, outputs_list)\n",
    "\n",
    "        train_loss = train_loss/train_steps\n",
    "        val_loss = val_loss/ test_steps\n",
    "        \n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"Epoch No\" , epoch)\n",
    "        print(\"The Training loss of the epoch, \",train_loss)\n",
    "        print(\"The Training AUC of the epoch,  %.5f\"%train_auc)\n",
    "        print(\"The validation loss of the epoch, \",val_loss)\n",
    "        print(\"The validation AUC of the epoch, %.5f\"%test_auc)\n",
    "        print(\"----------------------------------------------------\")\n",
    "        PATH = f\"model_Epoch_{epoch}.pt\"\n",
    "#         torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'scheduler': scheduler.state_dict()\n",
    "#                 }, PATH)\n",
    "        scheduler.step(test_auc)\n",
    "        curr_lr = scheduler._last_lr[0]\n",
    "        wandb.log({\"Train_auc_epoch\": train_auc,\n",
    "                  \"Epoch\": epoch,\n",
    "                  \"Val_auc_epoch\": test_auc,\n",
    "                  \"Train_loss_epoch\": train_loss,\n",
    "                  \"Val_loss_epoch\": val_loss,\n",
    "                  \"Lr\": curr_lr}\n",
    "                 )\n",
    "        gc.collect()\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdc250601\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diptarko/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/diptarko/G23/wandb/run-20230402_092932-azixgk4c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dc250601/Equivariant/runs/azixgk4c' target=\"_blank\">D8</a></strong> to <a href='https://wandb.ai/dc250601/Equivariant' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dc250601/Equivariant' target=\"_blank\">https://wandb.ai/dc250601/Equivariant</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dc250601/Equivariant/runs/azixgk4c' target=\"_blank\">https://wandb.ai/dc250601/Equivariant/runs/azixgk4c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [34:48<00:00,  1.20s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:18<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 0\n",
      "The Training loss of the epoch,  0.5807615502127286\n",
      "The Training AUC of the epoch,  0.76460\n",
      "The validation loss of the epoch,  0.5717705336110345\n",
      "The validation AUC of the epoch, 0.77521\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [34:58<00:00,  1.21s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:29<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 1\n",
      "The Training loss of the epoch,  0.5617238511127988\n",
      "The Training AUC of the epoch,  0.78401\n",
      "The validation loss of the epoch,  0.5612296441505695\n",
      "The validation AUC of the epoch, 0.78677\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [34:57<00:00,  1.21s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:29<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 2\n",
      "The Training loss of the epoch,  0.5559861934733117\n",
      "The Training AUC of the epoch,  0.78960\n",
      "The validation loss of the epoch,  0.5604981316232133\n",
      "The validation AUC of the epoch, 0.78865\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [35:01<00:00,  1.21s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:30<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 3\n",
      "The Training loss of the epoch,  0.5520208834916696\n",
      "The Training AUC of the epoch,  0.79324\n",
      "The validation loss of the epoch,  0.554742815850795\n",
      "The validation AUC of the epoch, 0.79155\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [34:57<00:00,  1.21s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:30<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 4\n",
      "The Training loss of the epoch,  0.5479065686293032\n",
      "The Training AUC of the epoch,  0.79703\n",
      "The validation loss of the epoch,  0.5594397825756292\n",
      "The validation AUC of the epoch, 0.79422\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [35:03<00:00,  1.21s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:30<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 5\n",
      "The Training loss of the epoch,  0.5441983305688562\n",
      "The Training AUC of the epoch,  0.80046\n",
      "The validation loss of the epoch,  0.5509804507096608\n",
      "The validation AUC of the epoch, 0.79577\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [34:57<00:00,  1.21s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:28<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 6\n",
      "The Training loss of the epoch,  0.5402696353265609\n",
      "The Training AUC of the epoch,  0.80408\n",
      "The validation loss of the epoch,  0.5510548414855168\n",
      "The validation AUC of the epoch, 0.79500\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [34:51<00:00,  1.20s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:28<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 7\n",
      "The Training loss of the epoch,  0.5355786830186844\n",
      "The Training AUC of the epoch,  0.80820\n",
      "The validation loss of the epoch,  0.5695691166938036\n",
      "The validation AUC of the epoch, 0.79406\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [34:31<00:00,  1.19s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:25<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 8\n",
      "The Training loss of the epoch,  0.5280031548320562\n",
      "The Training AUC of the epoch,  0.81468\n",
      "The validation loss of the epoch,  0.5695825976886969\n",
      "The validation AUC of the epoch, 0.79266\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [34:20<00:00,  1.18s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:24<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 9\n",
      "The Training loss of the epoch,  0.5170115168581064\n",
      "The Training AUC of the epoch,  0.82383\n",
      "The validation loss of the epoch,  0.5599908828735352\n",
      "The validation AUC of the epoch, 0.79076\n",
      "----------------------------------------------------\n",
      "Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1740/1740 [34:41<00:00,  1.20s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 435/435 [03:26<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Epoch No 10\n",
      "The Training loss of the epoch,  0.48044726853740627\n",
      "The Training AUC of the epoch,  0.85136\n",
      "The validation loss of the epoch,  0.5939084420258971\n",
      "The validation AUC of the epoch, 0.78201\n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████                     | 1280/1740 [25:32<09:08,  1.19s/it]"
     ]
    }
   ],
   "source": [
    "trainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
